### sigmoid
将一个实值输入压缩至[0,1]的范围。饱和激活函数。<br/>
缺点：梯度消失；不以0为中心；exp计算成本高。

![](https://latex.codecogs.com/svg.latex?\sigma(x)=\frac{1}{1+e^{-x}})

![](https://latex.codecogs.com/svg.latex?\begin{aligned}{\sigma}'(x)&=(1+e^{-x})^{-1}\\\\&=(-1){(1+e^{-x})}^{-2}e^{-x}(-1)\\\\&=\frac{e^{-x}}{(1+e^{-x})^2}\\\\&=\frac{1+e^{-x}-1}{(1+e^{-x})^2}\\\\&=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}\\\\&=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})\\\\&=\sigma(x)(1-\sigma(x))\end{aligned})

### tanh
将一个实值输入压缩至[-1,1]的范围。饱和激活函数。解决了sigmoid不以0为中心的问题。<br/>
缺点：梯度消失和exp计算问题仍然存在。

![](https://latex.codecogs.com/svg.latex?tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}})

![](https://latex.codecogs.com/svg.latex?\begin{aligned}{tanh}'(x)&=\frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}\\\\&=1-(\frac{e^x-e^{-x}}{e^x+e^{-x}})^2\\\\&=1-tanh^2(x)\end{aligned})

### relu（修正线性单元）
relu激活函数在0点处不可导，人为设置一个即可，一般设为0。非饱和激活函数。可以解决梯度消失，计算效率高。<br/>
缺点：不以0为中心；前向传导时，若x<0，神经元保持非激活状态，且在后向传播时“杀死”梯度，权重无法更新，网络无法更新。

![](https://latex.codecogs.com/svg.latex?relu(x)=max(0,x))

![](https://latex.codecogs.com/svg.latex?{relu}'(x)=\begin{cases}0,&x<0\\\\1,&x>0\end{cases})

### leaky relu
![](https://latex.codecogs.com/svg.latex?{lrelu}(x)=\begin{cases}0.01x,&x<0\\\\x,&x>0\end{cases})

### gelu（高斯误差线性单元）
gelu就是高斯误差线性单元，这种激活函数在激活中加入了随机正则的思想，是一种对神经元输入的概率描述。

![](https://latex.codecogs.com/svg.latex?xP(X\leq{x})=x\Phi(x)) <br/>

![](https://latex.codecogs.com/svg.latex?\Phi(x))指的是![](https://latex.codecogs.com/svg.latex?x)的标准正态分布的累积分布，完整形式如下：<br/>
![](https://latex.codecogs.com/svg.latex?xP(X\leq{x})=x\int_{-\infty}^{x}\frac{e^{-\frac{{(X-\mu)}^2}{2\sigma^2}}}{\sqrt{2\pi}\sigma}dX)

计算结果约为：<br/>
![](https://latex.codecogs.com/svg.latex?0.5x(1+tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^{3})))) <br/>
或者：<br/>
![](https://latex.codecogs.com/svg.latex?x\sigma(1.702x))

随着x的降低，它被归零的概率会升高。对于 ReLU来说，这个界限就是0，输入少于零就会被归零。gelu这一类激活函数，不仅保留了概率性，同时也保留了对输入的依赖性。

在一些典型的实验中，如MNIST图片分类任务，gelu的收敛速度明显快于relu。

Hugging Face的[gelu](https://github.com/huggingface/transformers/blob/master/src/transformers/activations.py)激活函数实现：
``` python
def _gelu_python(x):
    """
    Original Implementation of the GELU activation function in Google BERT repo when initially created. For
    information: OpenAI GPT's GELU is slightly different (and gives slightly different results): 0.5 * x * (1 +
    torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) This is now written in C in
    torch.nn.functional Also see the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    """
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))


def gelu_new(x):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    """
    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
```

参考资料
* [GAUSSIAN ERROR LINEAR UNITS (GELUS)](https://arxiv.org/pdf/1606.08415.pdf)
* [常见激活函数总结](https://zhuanlan.zhihu.com/p/192497127?utm_source=wechat_timeline)
