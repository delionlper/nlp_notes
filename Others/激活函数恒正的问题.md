假设神经网络：<br/>

![](https://latex.codecogs.com/svg.latex?y=f{(z)}=f{(\sum_i{w_i}{x_i})})

链式法则求导：<br/>

![](https://latex.codecogs.com/svg.latex?\frac{\partial{L}}{\partial{w_i}}=\frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{z}}\frac{\partial{z}}{\partial{w_i}}=\frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{z}}x_i)

参数更新：<br/>

![](https://latex.codecogs.com/svg.latex?w_i=w_i-\eta\frac{\partial{L}}{\partial{w_i}}=w_i-\eta\frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{z}}x_i)

* ![](https://latex.codecogs.com/svg.latex?\eta)是常量

* ![](https://latex.codecogs.com/svg.latex?\frac{\partial{L}}{\partial{f}})可正可负，对所有的![](https://latex.codecogs.com/svg.latex?w_i)来说是一样的，意味着符号相同

* ![](https://latex.codecogs.com/svg.latex?\frac{\partial{f}}{\partial{z}})是激活函数的导数，对于sigmoid来说是恒正的，推导见[这里](https://github.com/delionlper/nlp_notes/blob/main/Others/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.md)

* ![](https://latex.codecogs.com/svg.latex?{x}_i)是前一层的输出，如果前一层激活函数是sigmoid，那么是恒正的。

从以上分析来看，对所有的同一层的所有![](https://latex.codecogs.com/svg.latex?w_i)来说，其更新方向是一致的，模型为了收敛会走![](https://latex.codecogs.com/svg.latex?Z)字型逼近最优解。因此收敛速度会慢。
