来自NIPS 2017的一篇文章[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

![](transformer.jpg)

### Attention
* 将query和每个key进行相似度计算得到权重，常用的相似度函数有线性，加性，点积等
* 使用softmax对权重进行归一化
* 将权重和相对应的键值进行加权求和得到最后的Attention </br>

通常Key和Value是相同的。
![](attention.png)


### Scaled Dot-Product Attention
![](https://latex.codecogs.com/svg.latex?Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V) </br>
除以![](https://latex.codecogs.com/svg.latex?\sqrt{d_k})是为了方差为1，后续会有单独分析。

### Multi-Head Attention
![](https://latex.codecogs.com/svg.latex?head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)) </br>
![](https://latex.codecogs.com/svg.latex?MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O) </br>
这样做的好处是，可以允许模型在不同的表示子空间里学习到相关的信息（比如短语结构信息）。</br>
在实际的操作过程中，假设embedding size=768，heads=12，会沿着embedding size的维度分成12个head，每个head宽度是768/12=64。

### Positional Embedding
单独总结

### Add & Normalization
* BN <br/>
    主要应用场景在CNN中，针对N,C,H,W的输入，对每个C，在N,H,W三个维度上做均值和方差的归一化，N即是batch size。作用防止梯度消失，原因是能将神经元输出值拉回梯度变化明显的区域。<br/>
    BN的缺点：
    - 需要额外开辟内存缓存变量，空间消耗大
    - batch size小的时候效果不好，稳定性差
    - 不适合RNN这种动态长度的模型，batch中的长度不一致，靠后的输入不能算。实际可以把长度差不多的并入一个batch中
    - BN不适合NLP，[引用](https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/NLP%E4%BB%BB%E5%8A%A1%E4%B8%AD-layer-norm%E6%AF%94BatchNorm%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C.md)分析，一个batch中同一位置的不同单词，表示不同特征，在此维度归一化不合理。

* LN
针对N,H的输入，对每个N（即单个样本）在该层的输出（即隐藏层输出H）上做均值和方差的归一化。

### Position wise Feed Forward
![](https://latex.codecogs.com/svg.latex?FFN(x)=max(0,xW_1+b_1)W_2+b_2)

### Dropout
为了防止过拟合，实际上在Transformer结构中大量存在Dropout。

### TransformerEncoderLayer pytorch实现
<pre>
        src2 = self.self_attn(src, src, src, attn_mask=src_mask,key_padding_mask=src_key_padding_mask)[0] # self attention
        src = src + self.dropout1(src2) # residual connection
        src = self.norm1(src) # layer norm
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src)))) # dropout & feed forward
        src = src + self.dropout2(src2) # dropout & residual connection
        src = self.norm2(src) # layer norm
</pre>

### Transformer Decoder
* Masked Multi-Head Attention
实际操作过程中，是给那些mask的位置设一个非常大的负数，这样经过softmax后趋近于0
* Multi-Head Attention
和encoder端的Attention一样，唯一的区别是Query是Masked Multi-Head Attention的输出，Key和Value来自整个encoder的输出。

参考资料：
* [transformer 为什么使用 layer normalization，而不是其他的归一化方法](https://www.zhihu.com/question/395811291/answer/1260290120)
* [模型优化之Layer Normalization](https://zhuanlan.zhihu.com/p/54530247)
* [《Attention is All You Need》(2017NIPS)paper阅读笔记](https://zhuanlan.zhihu.com/p/34465668)
* [《attention is all you need》解读](https://zhuanlan.zhihu.com/p/34781297)